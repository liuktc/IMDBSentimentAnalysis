{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB dataset using Naive Bayes\n",
    "\n",
    "In this example, I'll try to implement the Naive Bayes algorithm to classify the sentiment of the IMDB dataset. The dataset contains 50k reviews, 25k for training and 25k for testing. The reviews are labeled as positive or negative.\n",
    "\n",
    "The idea behind this is to use a bag-of-words approach to represent the reviews and then use the Naive Bayes algorithm to classify them.\n",
    "\n",
    "Given a documend $d$ and a class $c$, the Naive Bayes algorithm calculates the probability of the class given the document as:\n",
    "\n",
    "$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "The model shoud return:\n",
    "\n",
    "$$\\hat{c} = \\arg\\max_c P(c|d) = \\arg\\max_c P(d|c) \\cdot P(c) $$\n",
    "\n",
    "If we represent the document with a set of features $f_1, \\dots, f_n$, the model can be simplified to:\n",
    "\n",
    "$$\\hat{c} = \\arg\\max_c P(c) \\prod_{i=1}^{n} P(f_i|c)$$\n",
    "\n",
    "By using a bag-of-words approach our features will simply be the words $w_i$ in the document. The model can be simplified to:\n",
    "\n",
    "$$\\hat{c} = \\arg\\max_c P(c) \\prod_{i=1}^{n} P(w_i|c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer # For Bag-Of-Words\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TARGET = \"sentiment\"\n",
    "dataset_filename = \"IMDB Dataset.csv\"\n",
    "df = pd.read_csv(dataset_filename)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3) \n",
    "df_test = df_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_train[TARGET].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probability(df, c):\n",
    "    return df[TARGET].value_counts()[c] / sum(df[TARGET].value_counts())\n",
    "\n",
    "def word_probability(X, features, w, X_sum, laplace_smoothing = False):\n",
    "    word_index = np.where(features == w)[0]\n",
    "    if len(word_index) == 0:\n",
    "        if laplace_smoothing:\n",
    "            return 1 / len(X)\n",
    "        else:\n",
    "            return 0\n",
    "    word_index = word_index[0]\n",
    "    \n",
    "    if laplace_smoothing:\n",
    "        return (X[word_index, 0] + 1) / (X_sum + len(X))\n",
    "    else:\n",
    "        return X[word_index, 0] / X_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [CountVectorizer(), CountVectorizer()]\n",
    "X = {\"positive\" : vectorizers[0].fit_transform(df_train[df_train[TARGET] == \"positive\"][\"review\"]),\n",
    "     \"negative\" : vectorizers[1].fit_transform(df_train[df_train[TARGET] == \"negative\"][\"review\"])}\n",
    "\n",
    "features = {\"positive\":vectorizers[0].get_feature_names_out(),\n",
    "            \"negative\":vectorizers[1].get_feature_names_out()}\n",
    "\n",
    "X = {\"positive\": np.sum(X[\"positive\"], axis=0).T,\n",
    "     \"negative\": np.sum(X[\"negative\"], axis=0).T}\n",
    "\n",
    "X_sum = {\"positive\": np.sum(X[\"positive\"], axis=0)[0,0],\n",
    "         \"negative\": np.sum(X[\"negative\"], axis=0)[0,0],}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical stability, we can use the log of the probabilities:\n",
    "\n",
    "$$\\hat{c} = \\arg\\max_c \\log P(c) + \\sum_{i=1}^{n} \\log P(w_i|c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_document(d, laplace_smoothing = False, tokenize=False, tokenizer=None):\n",
    "    if tokenize:\n",
    "        d = tokenizer(d)\n",
    "    likelihoods = {}\n",
    "    for c in [\"positive\", \"negative\"]:\n",
    "        res = np.log(class_probability(df, c))\n",
    "        \n",
    "        for word in d:\n",
    "            word_p = word_probability(X[c], features[c], word, X_sum[c], laplace_smoothing=True)\n",
    "            res += np.log(word_p)\n",
    "        \n",
    "        likelihoods[c] = res\n",
    "        \n",
    "    if likelihoods[\"positive\"] > likelihoods[\"negative\"]:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\"\n",
    "    # return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['really', 'loved', 'this', 'movie']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = vectorizers[0].build_tokenizer()\n",
    "document = \"I really loved this movie\"\n",
    "print(tokenizer(document))\n",
    "classify_document(tokenizer(document), laplace_smoothing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:52<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "correct = 0\n",
    "wrong = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    true_class = df_test[\"sentiment\"][i]\n",
    "    predicted_class = classify_document(df_test[\"review\"][i], laplace_smoothing=True, tokenize=True, tokenizer=tokenizer)\n",
    "    # print(f\"True class: {true_class}\")\n",
    "    # print(f\"Predicted class: {predicted_class}\")\n",
    "    if predicted_class == true_class:\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.811\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct / (correct + wrong)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This model, despite its semplicity, is able to achieve pretty good results. Of course, some limitations include the impossibility of understanding the context of a word (e.g. \"not good\" is considered positive). It can be used for sure as a baseline for more complex models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
